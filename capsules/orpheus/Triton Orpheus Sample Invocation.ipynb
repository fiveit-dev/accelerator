{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb8aaf8-b984-4c5f-aefb-8b1df53cdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c793e6-ef31-4227-a32a-3c00b68824bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2, service_pb2_grpc\n",
    "\n",
    "host = \"localhost:8001\"\n",
    "model_name = 'orpheus'\n",
    "model_version = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8d2726-4690-4843-adc7-33c7940b239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel(host)\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00e40b1-d6cb-4332-a816-2944f8377f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server live: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checks server health is ok!\n",
    "try:\n",
    "    request = service_pb2.ServerLiveRequest()\n",
    "    response = grpc_stub.ServerLive(request)\n",
    "    print(\"server {}\".format(response))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85acdb8e-ab6a-4fbf-8b40-517f3d297779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model metadata:\n",
      "name: \"orpheus\"\n",
      "versions: \"1\"\n",
      "platform: \"python\"\n",
      "inputs {\n",
      "  name: \"text\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: -1\n",
      "}\n",
      "inputs {\n",
      "  name: \"speaker_id\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: -1\n",
      "}\n",
      "outputs {\n",
      "  name: \"audio\"\n",
      "  datatype: \"FP32\"\n",
      "  shape: -1\n",
      "}\n",
      "outputs {\n",
      "  name: \"sampling_rate\"\n",
      "  datatype: \"FP32\"\n",
      "  shape: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checks model outputs\n",
    "request = service_pb2.ModelMetadataRequest(name=model_name, version=model_version)\n",
    "response = grpc_stub.ModelMetadata(request)\n",
    "print(\"model metadata:\\n{}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3cdee9-a4df-4a91-9487-473e72985a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Opening gRPC bidirectional stream â€¦\n",
      "ğŸš€ Sending request Â«ttsâ€‘4Â» (59 B text)\n",
      "ğŸ“¼ Opened output.wav  @  24000Â Hz â€” int16\n",
      "ğŸ”Š chunk 000 | 2048 samples |  1.32s since start\n",
      "ğŸ”Š chunk 001 | 2048 samples |  1.54s since start\n",
      "ğŸ”Š chunk 002 | 2048 samples |  1.75s since start\n",
      "ğŸ”Š chunk 003 | 2048 samples |  1.96s since start\n",
      "ğŸ”Š chunk 004 | 2048 samples |  2.18s since start\n",
      "ğŸ”Š chunk 005 | 2048 samples |  2.39s since start\n",
      "ğŸ”Š chunk 006 | 2048 samples |  2.61s since start\n",
      "ğŸ”Š chunk 007 | 2048 samples |  2.82s since start\n",
      "ğŸ”Š chunk 008 | 2048 samples |  3.03s since start\n",
      "ğŸ”Š chunk 009 | 2048 samples |  3.24s since start\n",
      "ğŸ”Š chunk 010 | 2048 samples |  3.46s since start\n",
      "ğŸ”Š chunk 011 | 2048 samples |  3.67s since start\n",
      "ğŸ”Š chunk 012 | 2048 samples |  3.88s since start\n",
      "ğŸ”Š chunk 013 | 2048 samples |  4.10s since start\n",
      "ğŸ”Š chunk 014 | 2048 samples |  4.31s since start\n",
      "ğŸ”Š chunk 015 | 2048 samples |  4.52s since start\n",
      "ğŸ”Š chunk 016 | 2048 samples |  4.74s since start\n",
      "ğŸ”Š chunk 017 | 2048 samples |  4.95s since start\n",
      "ğŸ”Š chunk 018 | 2048 samples |  5.17s since start\n",
      "ğŸ”Š chunk 019 | 2048 samples |  5.38s since start\n",
      "ğŸ”Š chunk 020 | 2048 samples |  5.60s since start\n",
      "ğŸ”Š chunk 021 | 2048 samples |  5.82s since start\n",
      "ğŸ”Š chunk 022 | 2048 samples |  6.03s since start\n",
      "ğŸ”Š chunk 023 | 2048 samples |  6.24s since start\n",
      "ğŸ”Š chunk 024 | 2048 samples |  6.46s since start\n",
      "ğŸ”Š chunk 025 | 2048 samples |  6.67s since start\n",
      "ğŸ”Š chunk 026 | 2048 samples |  6.89s since start\n",
      "ğŸ”Š chunk 027 | 2048 samples |  7.10s since start\n",
      "ğŸ”Š chunk 028 | 2048 samples |  7.32s since start\n",
      "ğŸ”Š chunk 029 | 2048 samples |  7.54s since start\n",
      "ğŸ”Š chunk 030 | 2048 samples |  7.75s since start\n",
      "ğŸ”Š chunk 031 | 2048 samples |  7.97s since start\n",
      "ğŸ”Š chunk 032 | 2048 samples |  8.18s since start\n",
      "ğŸ”Š chunk 033 | 2048 samples |  8.40s since start\n",
      "ğŸ”Š chunk 034 | 2048 samples |  8.61s since start\n",
      "ğŸ”Š chunk 035 | 2048 samples |  8.83s since start\n",
      "ğŸ”Š chunk 036 | 2048 samples |  9.04s since start\n",
      "ğŸ”Š chunk 037 | 2048 samples |  9.26s since start\n",
      "ğŸ”Š chunk 038 | 2048 samples |  9.47s since start\n",
      "ğŸ”Š chunk 039 | 2048 samples |  9.69s since start\n",
      "ğŸ”Š chunk 040 | 2048 samples |  9.91s since start\n",
      "ğŸ”Š chunk 041 | 2048 samples | 10.12s since start\n",
      "ğŸ”Š chunk 042 | 2048 samples | 10.34s since start\n",
      "ğŸ”Š chunk 043 | 2048 samples | 10.56s since start\n",
      "ğŸ”Š chunk 044 | 2048 samples | 10.77s since start\n",
      "ğŸ”Š chunk 045 | 2048 samples | 10.99s since start\n",
      "ğŸ”Š chunk 046 | 2048 samples | 11.21s since start\n",
      "ğŸ”Š chunk 047 | 2048 samples | 11.43s since start\n",
      "ğŸ”Š chunk 048 | 2048 samples | 11.64s since start\n",
      "ğŸ”Š chunk 049 | 2048 samples | 11.86s since start\n",
      "ğŸ”Š chunk 050 | 2048 samples | 12.08s since start\n",
      "ğŸ”Š chunk 051 | 2048 samples | 12.29s since start\n",
      "ğŸ”Š chunk 052 | 2048 samples | 12.51s since start\n",
      "ğŸ”Š chunk 053 | 2048 samples | 12.73s since start\n",
      "ğŸ”Š chunk 054 | 2048 samples | 13.99s since start\n",
      "ğŸ“¬  finalâ€‘response flag received\n",
      "âœ… Finished â€“ wrote 4.69Â s audio to output.wav\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verbose streaming client for a decoupled Triton TTS model (`orpheus`).\n",
    "\n",
    "â€¢ Sends one request with BYTES inputs:  text  +  speaker_id\n",
    "â€¢ Receives many responses, each carrying:\n",
    "      audio          : FP32 1â€‘D tensor\n",
    "      sampling_rate  : FP32 scalar (may appear only once)\n",
    "\n",
    "Each chunk is converted to 16â€‘bit PCM and appended to output.wav while\n",
    "progress messages stream to the console.\n",
    "\"\"\"\n",
    "import queue\n",
    "import wave\n",
    "from functools import partial\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONFIG\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_NAME  = \"orpheus\"\n",
    "TEXT        = \"Man, social media has completely changed how we interact...\"\n",
    "SPEAKER_ID  = \"tara\"\n",
    "REQUEST_ID  = \"ttsâ€‘4\"\n",
    "OUT_WAV     = \"output.wav\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# HELPERS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class _UserData:\n",
    "    \"\"\"Queue where the stream callback deposits every result / error.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.completed_reqs = queue.Queue()\n",
    "\n",
    "def _callback(user_data, result, error):\n",
    "    if error:\n",
    "        user_data.completed_reqs.put(error)\n",
    "    else:\n",
    "        user_data.completed_reqs.put(result)\n",
    "\n",
    "def _fp32_to_pcm16(float_chunk: np.ndarray) -> bytes:\n",
    "    \"\"\"\n",
    "    Convert float32 â€‘1â€¦1 â†’ int16 PCM.\n",
    "    The input array is readâ€‘only; make a clipped *copy* before scaling.\n",
    "    \"\"\"\n",
    "    tmp = np.clip(float_chunk, -1.0, 1.0)           # copy â€“ writable\n",
    "    return (tmp * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "def _is_final_response(resp) -> bool:\n",
    "    params = getattr(resp, \"parameters\", None)\n",
    "    if not params:\n",
    "        return False\n",
    "    flag = params.get(\"triton_final_response\")\n",
    "    return flag.bool_param\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# BUILD REQUEST\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "inp_text = grpcclient.InferInput(\"text\", [1], \"BYTES\")\n",
    "inp_text.set_data_from_numpy(np.array([TEXT.encode(\"utfâ€‘8\")], dtype=object))\n",
    "\n",
    "inp_spk  = grpcclient.InferInput(\"speaker_id\", [1], \"BYTES\")\n",
    "inp_spk.set_data_from_numpy(np.array([SPEAKER_ID.encode(\"utfâ€‘8\")], dtype=object))\n",
    "\n",
    "inputs  = [inp_text, inp_spk]\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(\"audio\"),\n",
    "    grpcclient.InferRequestedOutput(\"sampling_rate\"),\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OPEN STREAM â†’ SEND â†’ CONSUME\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "user_data   = _UserData()\n",
    "start_clock = perf_counter()\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as triton:\n",
    "    print(\"â³ Opening gRPC bidirectional stream â€¦\")\n",
    "    triton.start_stream(callback=partial(_callback, user_data))\n",
    "\n",
    "    print(f\"ğŸš€ Sending request Â«{REQUEST_ID}Â» ({len(TEXT)} B text)\")\n",
    "    triton.async_stream_infer(\n",
    "        model_name = MODEL_NAME,\n",
    "        inputs     = inputs,\n",
    "        outputs    = outputs,\n",
    "        request_id = REQUEST_ID,\n",
    "    )\n",
    "\n",
    "    wav_handle       = None\n",
    "    sampling_rate    = None\n",
    "    chunk_counter    = 0\n",
    "    pcm_frame_total  = 0\n",
    "\n",
    "    while True:\n",
    "        msg = user_data.completed_reqs.get()              # blocks\n",
    "\n",
    "        # â€” handle serverâ€‘side error â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "        if isinstance(msg, InferenceServerException):\n",
    "            raise msg\n",
    "\n",
    "        # â€” read tensors â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "        audio_np = msg.as_numpy(\"audio\")                 \n",
    "        if audio_np is None:\n",
    "            continue                                      \n",
    "\n",
    "        sr_arr = msg.as_numpy(\"sampling_rate\")            \n",
    "        if sr_arr is not None and sampling_rate is None:\n",
    "            sampling_rate = int(sr_arr[0])\n",
    "\n",
    "        # â€” open WAV lazily once we know the sampling rate â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "        if wav_handle is None and sampling_rate is not None:\n",
    "            sampwidth = audio_np.dtype.itemsize\n",
    "            wav_handle = wave.open(OUT_WAV, \"wb\")\n",
    "            wav_handle.setnchannels(1)\n",
    "            wav_handle.setsampwidth(sampwidth)\n",
    "            wav_handle.setframerate(sampling_rate)\n",
    "            print(f\"ğŸ“¼ Opened {OUT_WAV}  @  {sampling_rate}Â Hz â€” {audio_np.dtype}\")\n",
    "\n",
    "        # â€” convert + write chunk â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "        if wav_handle:\n",
    "            wav_handle.writeframes(audio_np)\n",
    "            pcm_frame_total += audio_np.size          \n",
    "\n",
    "        elapsed = perf_counter() - start_clock\n",
    "        print(\n",
    "            f\"ğŸ”Š chunk {chunk_counter:03d} | {len(audio_np)} samples\"\n",
    "            f\" | {elapsed:5.2f}s since start\"\n",
    "        )\n",
    "        chunk_counter += 1\n",
    "        \n",
    "        # â€” detect final response â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "        if _is_final_response(msg.get_response()):\n",
    "            print(\"ğŸ“¬  finalâ€‘response flag received\")\n",
    "            break\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DONE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if wav_handle:\n",
    "    wav_handle.close()\n",
    "    duration_sec = pcm_frame_total / sampling_rate\n",
    "    print(f\"âœ… Finished â€“ wrote {duration_sec:0.2f}Â s audio to {OUT_WAV}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No audio received!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ca816-b813-40e0-b92f-de8e4f4bff92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
